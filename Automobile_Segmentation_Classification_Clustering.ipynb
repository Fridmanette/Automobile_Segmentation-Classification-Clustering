{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6460700,"sourceType":"datasetVersion","datasetId":841888}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install xgboost\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, RandomizedSearchCV, learning_curve\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, confusion_matrix, classification_report, accuracy_score, silhouette_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.manifold import TSNE, LocallyLinearEmbedding, MDS\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as imPipeline\nfrom statsmodels.stats.anova import anova_lm\nfrom time import time\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.pipeline import Pipeline\nimport plotly.express as px","metadata":{"id":"OUDV343O7wi1","outputId":"754f320a-d4af-477d-e857-2255ca9b01a6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 1 - Classification","metadata":{}},{"cell_type":"markdown","source":"# 1.1. Preprocessing and EDA, Train and Test Data sets","metadata":{"id":"H-8oPajnZqQI"}},{"cell_type":"markdown","source":"Functions for data preprocessing, EDA, and Multicollinearity","metadata":{"id":"_lRDS63fe6cK"}},{"cell_type":"code","source":"def load_data(filename):\n    df = pd.read_csv(filename)\n    return df","metadata":{"id":"nBZEm0dCOPnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_data_info(df):\n    df.info()\n    print(df.describe())","metadata":{"id":"SYDgkbSmOPnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detecting and counting empty cells, NaN, and Inf\ndef detect_and_count_issues(df):\n    for column in df.columns:\n        if df[column].dtype in [np.float64, np.float32, np.int64, np.int32]:\n            # Only check for NaN and Inf in numeric columns\n            nan_count = df[column].isna().sum()\n            inf_count = np.isinf(df[column]).sum()\n            empty_count = 0\n        else:\n            # For non-numeric columns, only checking for NaN and empty cells\n            nan_count = df[column].isna().sum()\n            inf_count = 0  # No Inf check for non-numeric data\n            empty_count = (df[column] == '').sum()\n\n        # Print the counts for the column\n        print(f\"Column: {column}\")\n        print(f\"  NaN: {nan_count}\")\n        print(f\"  Inf: {inf_count}\")\n        print(f\"  Empty: {empty_count}\\n\")\n","metadata":{"id":"vm4U6j4IBK8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying duplicate values in the 'ID' column\ndef check_duplicates_in_id(df):\n    # Check for duplicate entries in the 'ID' column\n    duplicates = df[df.duplicated('ID')]\n    num_duplicates = len(duplicates)\n    print(f'Number of Duplicate Entries in ID: {num_duplicates}')\n","metadata":{"id":"LYJtE0G_OPnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputation for numerical variables by entering median value into empty, NaN, and Inf cells\ndef impute_numerical_median(df, col):\n    if col in df.columns:\n        median = df[col].median()\n        if median:\n            print(f\"median for '{col}' is: '{median}'\")\n            df[col] = df[col].fillna(median)\n            # Optionally, handle Inf values if they are present\n            df[col] = df[col].replace([np.inf, -np.inf], median)\n        else:\n            print(f\"Column '{col}' not numerical.\")\n    else:\n        print(f\"Column '{col}' does not exist in the DataFrame.\")\n    return df","metadata":{"id":"kXHiKS-rB9tl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing categorical variables using mode\ndef impute_categorical_mode(df, col):\n    if col in df.columns:\n        mode = df[col].mode()[0]\n        if mode:\n            df[col] = df[col].fillna(mode)\n            df[col] = df[col].replace('', mode)\n        else:\n            print(f\"Column '{col}' not categorical.\")\n    else:\n        print(f\"Column '{col}' does not exist in the DataFrame.\")\n    return df","metadata":{"id":"QP6jzFHHCFfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variable One-hot coding for categorical variables, this will applied to the 'Profession' variable\ndef transform_column(df, col):\n    if col in df.columns:\n        # One-hot encoding the specified column\n        df = pd.get_dummies(df, columns=[col], prefix=col)\n\n    else:\n        print(f\"Column '{col}' does not exist in the DataFrame.\")\n    return df","metadata":{"id":"WhDeSOCUCHDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check point function - Converting to numeric values\n # This was designed mainly to resolve an issue that can occur when One-hot coding results in True/False values instead of 0 and 1\ndef convert_to_numeric_columns(df):\n    for column in df.columns:\n        df[column] = pd.to_numeric(df[column], errors='coerce')\n    return df","metadata":{"id":"1PCMQCoaNInN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Check point function - Converting boolean columns to integers\n # This was designed mainly to resolve an issue that can occur when One-hot coding results in True/False values instead of 0 and 1\ndef bool_to_int(df):\n    for col_bool in df.select_dtypes(include=[bool]).columns:\n        df[col_bool] = df[col_bool].astype(int)\n\n    return df","metadata":{"id":"3ow94dpf_8Vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check point function - Verifying all columns are numeric\ndef verify_numeric_columns(df):\n    for column in df.columns:\n        if not np.issubdtype(df[column].dtype, np.number):\n            raise ValueError(f\"Column '{column}' is not numeric after transformation.\")\n","metadata":{"id":"Y4KioiCmJZQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing distributions of numerical features\ndef visualize_numerical_features(df):\n    numerical_features = ['Age', 'Work_Experience', 'Family_Size']\n    n_cols = 3  # Number of plots per row\n    n_rows = (len(numerical_features) + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(numerical_features):\n        sns.boxplot(x=df[feature], ax=axes[i])\n        axes[i].set_title(f'Box Plot of {feature}')\n        axes[i].set_xlabel(feature)\n        axes[i].set_ylabel('')\n\n    for i in range(len(numerical_features), len(axes)):\n        fig.delaxes(axes[i])\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"zN0SD_X5OPnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the distribution of categorical features\ndef visualize_categorical_features(df):\n    categorical_features = ['Gender','Ever_Married', 'Graduated', 'Profession', 'Spending_Score', 'Var_1', 'Segmentation']\n    n_cols = 3  # Number of plots per row\n    n_rows = (len(categorical_features) + n_cols - 1) // n_cols\n\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(categorical_features):\n        sns.countplot(x=feature, data=df, ax=axes[i])\n        axes[i].set_title(f'Distribution of {feature}')\n        axes[i].set_xlabel(feature)\n        axes[i].set_ylabel('Count')\n\n    # Removing any empty subplots\n    for i in range(len(categorical_features), len(axes)):\n        fig.delaxes(axes[i])\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"nVmMtMQQOPnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For visualizing customer segmentation for Catgorical and Numerical Variable\n\ndef plot_category(feature, df, figsize=None):\n    A_count = df[df['Segmentation']=='A'].groupby([feature]).size()\n    B_count = df[df['Segmentation']=='B'].groupby([feature]).size()\n    C_count = df[df['Segmentation']=='C'].groupby([feature]).size()\n    D_count = df[df['Segmentation']=='D'].groupby([feature]).size()\n    labels = A_count.index\n\n    x = np.arange(len(labels))  # the label locations\n    width = 0.7  # the width of the bars\n\n    if figsize:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig, ax = plt.subplots()\n\n    total_counts = df.groupby([feature]).size()\n\n    rects1 = ax.bar(x - width/3, round(A_count * 100 / total_counts, 2), width/5, label='A')\n    rects2 = ax.bar(x - width/8, round(B_count * 100 / total_counts, 2), width/5, label='B')\n    rects3 = ax.bar(x + width/8, round(C_count * 100 / total_counts, 2), width/5, label='C')\n    rects4 = ax.bar(x + width/3, round(D_count * 100 / total_counts, 2), width/5, label='D')\n\n    ax.set_ylabel('Count')\n    ax.set_title(f'Based on {feature}')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels, rotation=80)\n    ax.legend(loc=0, bbox_to_anchor=(1, 1))\n\n    ax.bar_label(rects1, padding=1)\n    ax.bar_label(rects2, padding=1)\n    ax.bar_label(rects3, padding=1)\n    ax.bar_label(rects4, padding=1)\n\n    fig.tight_layout()\n    plt.show()\n\ndef plot_numerical(feature, df, figsize=None):\n    fig = plt.figure(figsize=(10, 6))\n\n    sns.kdeplot(df[df['Segmentation']=='A'][feature], label='Segmentation A')\n    sns.kdeplot(df[df['Segmentation']=='B'][feature], label='Segmentation B')\n    sns.kdeplot(df[df['Segmentation']=='C'][feature], label='Segmentation C')\n    sns.kdeplot(df[df['Segmentation']=='D'][feature], label='Segmentation D')\n\n    plt.title(f'Based on {feature}')\n    plt.legend()\n    plt.show()\n\ndef plot_pie(feature, df):\n    plot_data = df.groupby([feature, 'Segmentation']).size().reset_index(name='count')\n\n    fig = px.sunburst(plot_data, path=[feature, 'Segmentation'], values='count', color=feature,\n                      title=f'Affect of {feature} on Customer Segmentation', width=400, height=400)\n\n    fig.update_layout(plot_bgcolor='white', title_font_family='Calibri Black', title_font_color='#092e5b',\n                      title_font_size=20, title_x=0.5)\n\n    fig.update_traces(textinfo='label+percent parent')\n    fig.show()","metadata":{"id":"EkmDDk0O0EHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping Categorical Variables\ndef cat_to_num(df):\n\n    # Transforming 'Gender' column\n    df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n\n    # Transforming 'Ever_Married' column\n    df['Ever_Married'] = df['Ever_Married'].map({'No': 0, 'Yes': 1})\n\n    # Transforming 'Graduated' column\n    df['Graduated'] = df['Graduated'].map({'No': 0, 'Yes': 1})\n\n    # Transforming 'Spending_Score' column to numeric\n    df['Spending_Score'] = df['Spending_Score'].map({'Low': 1, 'Average': 2, 'High': 3})\n\n    # Converting boolean values to integers (0s and 1s)\n    for col in df.select_dtypes(include=[bool]).columns:\n        df[col] = df[col].astype(int)\n\n    # If 'Var_1' is a categorical variable, convert it to numeric\n    df['Var_1'] = df['Var_1'].astype('category').cat.codes\n\n    # Ensuring 'Segmentation' is converted to numeric\n    df['Segmentation'] = df['Segmentation'].astype('category').cat.codes\n\n    # Verifying the new columns\n    print('After transformation:')\n    print(df.head())\n\n    # Printing the data types of each column\n    print(\"\\nData types of each column:\")\n    print(df.dtypes)\n\n    return df","metadata":{"id":"ZLhRKpAWOPnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling outliers by IQR\n# This is to assigned to specific numerical variables that may contain outliers\ndef handle_outliers_iqr(df):\n    # Specifying the numeric columns to check for outliers\n    numerical_features = ['Age', 'Work_Experience', 'Family_Size']\n\n    outliers_info = {}\n    for feature in numerical_features:\n        if feature in df.columns:\n            Q1 = df[feature].quantile(0.25)\n            Q3 = df[feature].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n\n            # Counting the number of outliers\n            num_outliers = ((df[feature] < lower_bound) | (df[feature] > upper_bound)).sum()\n            outliers_info[feature] = num_outliers\n\n            # Removing the outliers\n            df = df[~((df[feature] < lower_bound) | (df[feature] > upper_bound))]\n\n    return df, outliers_info","metadata":{"id":"xPD666jpOPnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation Matrix Heatmap\ndef compute_correlation_matrix(df):\n    plt.figure(figsize=(20, 15))\n    correlation_matrix = df.corr()\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.title('Correlation Matrix')\n    plt.show()","metadata":{"id":"isauAo0IOPnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ANOVA Analysis\ndef run_anova(df):\n    categorical_vars_corrected = ['Gender','Ever_Married', 'Graduated', 'Spending_Score', 'Var_1', 'Profession_Artist', 'Profession_Doctor', 'Profession_Engineer', 'Profession_Entertainment',\n                                  'Profession_Executive', 'Profession_Healthcare', 'Profession_Homemaker', 'Profession_Lawyer', 'Profession_Marketing']\n\n    anova_results = {}\n    for var in categorical_vars_corrected:\n        model_anova = ols(f'Segmentation ~ C({var})', data=df).fit()\n        anova_table = anova_lm(model_anova, typ=2)\n        anova_results[var] = anova_table\n\n    print(\"ANOVA Analysis Results:\")\n    for var, results in anova_results.items():\n        print(f\"\\nANOVA for {var}:\\n\", results)","metadata":{"id":"Be7ieRdqOPnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating VIF for detecting multicollinearity\ndef calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data['Feature'] = df.columns\n    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n    print(\"Variance Inflation Factor (VIF):\")\n    print(vif_data)","metadata":{"id":"i_iwh54fOPnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check point - function - Any Missing Values still?\ndef missing_values_after_imputation(df):\n    missing_values_after_imputation = df.isnull().sum()\n    print(\"Missing values after imputation:\\n\", missing_values_after_imputation)\n\n    return df","metadata":{"id":"4xVbRhX-cnrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing 'ID' column\ndef remove_id(df):\n    # Drop the 'ID' column\n    if 'ID' in df.columns:\n        df = df.drop(columns=['ID'])\n    else:\n        print(\"ID column not found.\")\n    return df","metadata":{"id":"exn2wl8qeNF6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combining functions to prepare datasets for analysis and visualization","metadata":{"id":"o1v61KdufMBg"}},{"cell_type":"code","source":"''' This is a global function designed to handle all initial preparing of both datasets:\n- loading the datasets\n- Displaying number of empty, NaN and Inf values in each column\n- Detecting duplicates in 'ID'\n- Imputing numerical variables by median and categorical variables by mode\n- Visualizing distributions of numerical features\n- Visualizing the distribution of categorical features\n- One-Hot coding, to applied to 'Profession'\n- Converting to numeric values\n- Converting boolean columns to integers\n- Verifying all columns are numeric\n- Final validation of data integrity '''\n\ndef datasets_1(filename):\n    df = load_data(filename)\n    display_data_info(df)\n    detect_and_count_issues(df)\n    check_duplicates_in_id(df)\n    df = impute_numerical_median(df, 'Age')\n    df = impute_numerical_median(df, 'Work_Experience')\n    df = impute_numerical_median(df, 'Family_Size')\n    df = impute_categorical_mode(df, 'Ever_Married')\n    df = impute_categorical_mode(df, 'Graduated')\n    df = impute_categorical_mode(df, 'Profession')\n    df = impute_categorical_mode(df, 'Var_1')\n    detect_and_count_issues(df)\n    display_data_info(df)\n    missing_values_after_imputation(df)\n\n    return df\n\n# EDA\ndef datasets_2(df):\n    visualize_numerical_features(df)\n    visualize_categorical_features(df)\n\ndef customersegment1(df):\n    for feature in ['Gender', 'Ever_Married', 'Graduated', 'Spending_Score']:\n        plot_pie(feature, df)\n\ndef customersegment2(df):\n    for feature in ['Profession', 'Var_1']:\n        plot_pie(feature, df)\n\ndef customersegment3(df):\n    for feature in ['Age', 'Work_Experience', 'Family_Size']:\n        plot_numerical(feature, df)\n\n\n# This one converts categorical to numeric, One-hot coding for 'Profession', verifying data integrity\ndef datasets_3(df):\n    df = cat_to_num(df)\n    df = transform_column(df, 'Profession')\n    df = convert_to_numeric_columns(df)\n    df = bool_to_int(df)\n    verify_numeric_columns(df)\n    detect_and_count_issues(df)\n\n    return df\n\n\n# This one is for visualizations after converting all values to numeric, here for correlation matrix\ndef analyses(df):\n    compute_correlation_matrix(df)\n\n# Analyzing multicollinearity\ndef multicollinearity(df):\n    run_anova(df)\n    calculate_vif(df)","metadata":{"id":"jrZ3s6-6GDDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading and defining dataframes for train and test, detecting nulls, NaN, Inf, imputation\ndf_train = datasets_1('Train.csv')\ndf_test = datasets_1('Test.csv')","metadata":{"id":"30er5qYh9ODz","outputId":"9985b011-fb7d-4ec5-cf97-a76db6db78b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_data_info(df_train)","metadata":{"id":"xh78rfe7C4qp","outputId":"e542f104-4230-4477-c6b7-63772558d1b9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box plots and bar plots for both datasets\ndatasets_2(df_train)\ndatasets_2(df_test)","metadata":{"id":"LztIyd9wP5H_","outputId":"fdb849b2-e5cd-4a2f-9450-1c65e12c176d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting categorical to numeric, One-hot coding for 'Profession', verifying data integrity\ndf_train = datasets_3(df_train)\ndf_test = datasets_3(df_test)","metadata":{"id":"EmPvpemRP5Aw","outputId":"76edfa4c-10fa-4ab9-cd70-c526e5187ccd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data validation\ndisplay_data_info(df_train)\ndisplay_data_info(df_test)","metadata":{"id":"zYtx4rhNP447","outputId":"d859c85d-2971-4b23-ff87-3eaae85690b5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling outliers, Train\ndf_train, outliers_info = handle_outliers_iqr(df_train)\nprint(\"Number of outliers detected and removed per column:\")\nprint(outliers_info)","metadata":{"id":"Qbdarc9OP4yN","outputId":"32b0062f-76d9-4874-9a9c-adacb898f39d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_data_info(df_train)","metadata":{"id":"ve8SCVFuP4pw","outputId":"c2645eaf-6a1f-49d6-c1cd-bef50ee9fbaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling outliers, Test\ndf_test, outliers_info = handle_outliers_iqr(df_test)\nprint(\"Number of outliers detected and removed per column:\")\nprint(outliers_info)","metadata":{"id":"X2E7OvpDdfK1","outputId":"d4fb31c0-15a8-411e-8da0-785964de85ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_data_info(df_test)","metadata":{"id":"kIMQOIpPdk5j","outputId":"77c635e0-d475-429c-83a7-cf88812aa733"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix heatmap\ndf_Matrix_train = df_train.copy()\ndf_Matrix_test = df_test.copy()\n\n# Removing 'ID' column from the copied dataframes\nif 'ID' in df_Matrix_train.columns:\n    df_Matrix_train = df_Matrix_train.drop(columns=['ID'])\nif 'ID' in df_Matrix_test.columns:\n    df_Matrix_test = df_Matrix_test.drop(columns=['ID'])\n\n# Running the analyses function on the copied dataframes\nanalyses(df_Matrix_train)\nanalyses(df_Matrix_test)","metadata":{"id":"dMZYSRf_adAn","outputId":"5478da76-d95a-4a50-dde8-34c40503e262"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Detecting Multicollinearity\n","metadata":{"id":"enzwrNmsaIvR"}},{"cell_type":"code","source":"# ANOVA and Multicollinearity by computing VIFs\nmulticollinearity(df_train)","metadata":{"id":"DVgE2JfjbDPS","outputId":"253e7e46-2339-434d-afa1-8caa050cc11b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ANOVA and Multicollinearity by computing VIFs\nmulticollinearity(df_test)","metadata":{"id":"faeISl-oAy2b","outputId":"6f664d79-a33e-483a-c672-d65392044998"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying the smallest profession group\nprofession_columns = [col for col in df_train.columns if col.startswith('Profession_')]\nsmallest_group = df_train[profession_columns].sum().idxmin()\nprint(f'Smallest profession group: {smallest_group}')\n\n# Removing the smallest profession group from both training and testing datasets\ndf_train = df_train.drop(columns=[smallest_group])\ndf_test = df_test.drop(columns=[smallest_group])\n\nprint(f\"Dropped column: {smallest_group}\")\n\n# Recalculate VIF\ncalculate_vif(df_train.drop(columns=['Segmentation']))","metadata":{"id":"bd2e8e83","outputId":"6eea2151-60f1-4290-868a-a39c930da87c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VIF >= 10 as cutoff for multicollinearirty?\n# Thus, due to high VIF, we will remove 'Age' from the datasets\ndf_train = df_train.drop(columns=['Age'])\ndf_test = df_test.drop(columns=['Age'])\n\nprint(\"Dropped column: Age\")\n\n# Recalculating VIF\ncalculate_vif(df_train.drop(columns=['Segmentation']))","metadata":{"id":"45de4be2","outputId":"ffcb2f52-1987-44aa-f905-a8f819fd8dfc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing 'ID' Column from both datasets\ndf_train = remove_id(df_train)\ndf_test = remove_id(df_test)","metadata":{"id":"aIuDwJ7xeJs4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recalculating VIF\ncalculate_vif(df_train.drop(columns=['Segmentation']))","metadata":{"id":"XVAndZ4LDTHJ","outputId":"9ce74a4e-8980-4927-ccfa-ed2209811cb5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, both datasets have been preprocessed and are ready for classification models!","metadata":{"id":"NkdXx-Gx8m8q"}},{"cell_type":"markdown","source":"# 1.2. Preparing Datasets for Modeling","metadata":{"id":"0CZqpZGOeixi"}},{"cell_type":"markdown","source":"Defining Train and Test","metadata":{"id":"32rUaQ9iaoj2"}},{"cell_type":"code","source":"# Defining train and target for modeling, with tarin transformed dataset (df_train) as training, test transformed dataset (df_test) as test\nX_train = df_train.drop(columns=['Segmentation'])\ny_train = df_train['Segmentation']\n\nX_test = df_test.drop(columns=['Segmentation'])\ny_test = df_test['Segmentation']\n","metadata":{"id":"87553be2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Functions for Confusion Matrices and Learning Curves","metadata":{"id":"nIi7UCjvauZK"}},{"cell_type":"code","source":"# For modeling - Functions for confusion matrices, learning curves\ndef plot_confusion_matrix(cm, title, save_path=None):\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    if save_path:\n        plt.savefig(save_path)\n    plt.show()\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), scoring='accuracy'):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    # Calculating learning curve\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n\n    # Calculating mean and standard deviation for training and test scores\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plotting learning curve\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n\n    plt.show()\n    # return plt","metadata":{"id":"36ff1d2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Selection by Ridge and Lasso Regression","metadata":{"id":"wquMAmhta6ba"}},{"cell_type":"code","source":"# Lasso Regression for feature selection\n\n# Standardizing the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Lasso Regression\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train_scaled, y_train)\nlasso_pred = lasso.predict(X_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_pred)\nlasso_r2 = r2_score(y_test, lasso_pred)\nprint(f\"Lasso Regression MSE: {lasso_mse}\")\nprint(f\"Lasso Regression R^2: {lasso_r2}\")\n\n# Feature Importance from Lasso Regression\nlasso_coefficients = pd.Series(lasso.coef_, index=X_train.columns)\nlasso_coefficients = lasso_coefficients[lasso_coefficients != 0]\nprint(f\"Lasso selected {len(lasso_coefficients)} features from {X_train.shape[1]}\")\nprint(\"Selected features and their coefficients:\")\nprint(lasso_coefficients)\n\n# Plotting Lasso Feature Importance (absolute values to avoid negative importance)\nplt.figure(figsize=(10, 6))\nlasso_coefficients.abs().sort_values().plot(kind='barh', color='blue')\nplt.title('Feature Importance from Lasso Regression')\nplt.xlabel('Absolute Coefficient Value')\nplt.ylabel('Features')\nplt.show()\n\n# Further Evaluation with Cross-Validation\nlasso_cv_scores = cross_val_score(lasso, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(f\"Lasso Regression CV MSE: {-np.mean(lasso_cv_scores)}\")","metadata":{"id":"8be569a0","outputId":"05c0a062-4f02-45b8-d603-375a1eb3747f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.3. Classification models\n\nPCA Dimensionality Reduction\n\nClassifiers and Hyperparameter tuning\n\nEnsemble Models\n\nSVM\n\nClass Imbalance\n\nFeature Importance\n\nLearning Curves\n\nEvaluating all models","metadata":{"id":"Io4wnpfEbCwj"}},{"cell_type":"markdown","source":"Pipeline with PCA and DecisionTree Classifier","metadata":{"id":"oKZU8TaobUNH"}},{"cell_type":"code","source":"# Creating a pipeline with PCA for dimensionality reduction, DecisionTree Classifer\npipeline = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=0.95)),  # Retain 95% of the variance\n    ('classifier', DecisionTreeClassifier(random_state=42))\n])\n\n# Training and evaluating the pipeline\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\nprint(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred)}\")\ncm = confusion_matrix(y_test, y_pred)\nprint(f\"Confusion Matrix:\\n{cm}\")\nprint(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\nplot_confusion_matrix(cm, \"Confusion Matrix for Decision Tree\")","metadata":{"id":"e331416f","outputId":"b6b5e202-b87d-4bac-f44b-3af41ad1096a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GridSearchCV Hyperparameter Tuning","metadata":{"id":"XvaB3LrPbeFg"}},{"cell_type":"code","source":"# Hyperparameters tuning with GridSearchCV\n# Defining the parameter grid\nparam_grid = {\n    'classifier__max_depth': [5, 10, 15],\n    'classifier__min_samples_split': [2, 5, 10]\n}\n\n# Creating the GridSearchCV object\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# Fitting the model and finding the best parameters\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\nprint(f\"Best Parameters: {best_params}\")\nprint(f\"Best Cross-validation Accuracy: {best_score}\")\n\n# Evaluating on test set\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nplot_confusion_matrix(cm, \"Confusion Matrix for GridSearchCV Best Model\")","metadata":{"id":"0eef9be7","outputId":"d4549910-a6e0-4e34-afe4-58448b50aabf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RandomizedSearchCV Hyperparameter Tuning","metadata":{"id":"jLZJ-nHpbl7d"}},{"cell_type":"code","source":"# Hyperparameters tuning with RandomizedSearchCV\n# Defining the parameter grid\nparam_dist = {\n    'classifier__max_depth': [5, 10, 15, 20],\n    'classifier__min_samples_split': [2, 5, 10, 15],\n    'classifier__min_samples_leaf': [1, 2, 5, 10]\n}\nrandom_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy', random_state=42)\nrandom_search.fit(X_train, y_train)\n\n# Best parameters\nbest_params = random_search.best_params_\nbest_score = random_search.best_score_\nprint(f\"Best Parameters: {best_params}\")\nprint(f\"Best Cross-validation Accuracy: {best_score}\")\n\n# Evaluating on test set\nbest_model = random_search.best_estimator_\ny_pred = best_model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nplot_confusion_matrix(cm, \"Confusion Matrix for RandomizedSearchCV Best Model\")","metadata":{"id":"7e15d369","outputId":"1e503a64-59f9-43e4-bbad-d0b1c29b940a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensemble Models","metadata":{"id":"C92_S48Lbu4C"}},{"cell_type":"code","source":"# Ensemble models\n# Defining the ensemble models\nensemble_models = {\n    'AdaBoost': AdaBoostClassifier(random_state=42),\n    'Gradient Boosted Trees': GradientBoostingClassifier(random_state=42),\n    'Random Forest': RandomForestClassifier(random_state=42)\n}\n\n# Training and evaluating models\nfor model_name, model in ensemble_models.items():\n    # No PCA in the pipeline in order to plot feature importance correctly\n    pipeline = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('classifier', model)\n    ])\n\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    print(f\"Model: {model_name}\")\n    print(f\"Accuracy: {accuracy}\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\nClassification Report:\\n\", report)\n    print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n    # Plotting feature importance for ensemble models\n    if hasattr(model, 'feature_importances_'):\n        feature_importances = model.feature_importances_\n\n        # Sorting feature importances in descending order\n        indices = np.argsort(feature_importances)[::-1]\n\n        # Rearranging feature names so they match the sorted feature importances\n        names = [X_train.columns[i] for i in indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.title(f\"Feature Importance for {model_name}\")\n        plt.bar(range(X_train.shape[1]), feature_importances[indices])\n        plt.xticks(range(X_train.shape[1]), names, rotation=90)\n        plt.xlabel('Features')\n        plt.ylabel('Importance Score')\n        plt.show()\n\n        # Display the importance scores\n        print(\"Feature importances:\")\n        for name, importance in zip(names, feature_importances[indices]):\n            print(f\"{name}: {importance:.4f}\")","metadata":{"id":"cdd5a36b","outputId":"d5c1cfe0-481e-4ad6-c4f4-0efa262aba3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Learning Curves for AdaBoost and Gradient Boosted Trees","metadata":{"id":"Emw9o7zWcKx2"}},{"cell_type":"code","source":"# Plotting learning curves for AdaBoost and Gradient Boosted Trees\nplot_learning_curve(AdaBoostClassifier(random_state=42), \"Learning Curve (AdaBoost)\", X_train, y_train, cv=5, n_jobs=-1)\nplot_learning_curve(GradientBoostingClassifier(random_state=42), \"Learning Curve (Gradient Boosted Trees)\", X_train, y_train, cv=5, n_jobs=-1)\n","metadata":{"id":"0fde0b47","outputId":"97a39e4b-6880-459a-e3a4-bd6455dbdefd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Support Vector Machine (SVM) Model","metadata":{"id":"JEhK-w1ib2Sg"}},{"cell_type":"code","source":"# Defining the SVM model parameters\nsvm = SVC(kernel='linear', random_state=42, probability=True)\n\n# Creating a pipeline\npipeline = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=0.95)),  # Retain 95% of the variance\n    ('classifier', svm)\n])\n\n# Training and evaluating the SVM model\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(f\"Model: Support Vector Machine (SVM)\")\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\\n\", report)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")","metadata":{"id":"eb503366","outputId":"5087ad12-9f3f-4fb2-bd18-a930aa8e0e35"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Treating Class Imbalance","metadata":{"id":"YUx7PaDOcUWy"}},{"cell_type":"code","source":"# Addressing possible class imbalance\n# Creating pipeline with SMOTE\npipeline = imPipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=0.95)),  # Retain 95% of the variance\n    ('smote', SMOTE(random_state=42)),\n    ('classifier', DecisionTreeClassifier(random_state=42))\n])\n\n# Training the model\npipeline.fit(X_train, y_train)\n\n# Predicting and evaluating\ny_pred = pipeline.predict(X_test)\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred), \"Confusion Matrix with SMOTE\")\n\n\n# Creating pipeline with RandomUnderSampler\nundersample_pipeline = imPipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=0.95)),  # Retain 95% of the variance\n    ('undersample', RandomUnderSampler(random_state=42)),\n    ('classifier', DecisionTreeClassifier(random_state=42))\n])\n\n# Training the model with RandomUnderSampler\nundersample_pipeline.fit(X_train, y_train)\n\n# Predicting and evaluating with RandomUnderSampler\ny_pred_undersample = undersample_pipeline.predict(X_test)\nprint(\"Confusion Matrix with RandomUnderSampler:\")\nprint(confusion_matrix(y_test, y_pred_undersample))\nprint(\"\\nClassification Report with RandomUnderSampler:\\n\", classification_report(y_test, y_pred_undersample))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred_undersample), \"Confusion Matrix with RandomUnderSampler\")\n\n\n# Using class weights in the DecisionTreeClassifier\nclass_weight_pipeline = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=0.95)),  # Retain 95% of the variance\n    ('classifier', DecisionTreeClassifier(class_weight='balanced', random_state=42))\n])\n\n# Training the model with class weights\nclass_weight_pipeline.fit(X_train, y_train)\n\n# Predicting and evaluating with class weights\ny_pred_class_weight = class_weight_pipeline.predict(X_test)\nprint(\"Confusion Matrix with Class Weights:\")\nprint(confusion_matrix(y_test, y_pred_class_weight))\nprint(\"\\nClassification Report with Class Weights:\\n\", classification_report(y_test, y_pred_class_weight))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred_class_weight), \"Confusion Matrix with Class Weights\")","metadata":{"id":"09a4eda8","outputId":"f59c65df-5628-4e83-b8fb-34913a623191"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Importance","metadata":{"id":"SKjs4yencgbF"}},{"cell_type":"code","source":"# Feature Importance with Permutation Importance\n# Fitting a model directly on scaled data\nrf_original = RandomForestClassifier(random_state=42)\nrf_original.fit(X_train_scaled, y_train)  # X_train_scaled is from original features\n\n# Assessing permutation importance\nresults = permutation_importance(rf_original, X_train_scaled, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n\n# Plotting the results\nsorted_idx = results.importances_mean.argsort()\nplt.figure(figsize=(10, 8))\nplt.boxplot(results.importances[sorted_idx].T, vert=False, labels=X_train.columns[sorted_idx])\nplt.title(\"Permutation Feature Importance\")\nplt.show()","metadata":{"id":"3ecb55cd","outputId":"739d915a-b04e-4c8a-f009-35ede1b9b3e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.4. Summarizing and Comparing Models","metadata":{"id":"Dz5Ttuitckwb"}},{"cell_type":"code","source":"# Final evaluation of all models with the best parameters and techniques (class imbalance excluded)\n\n# Best parameters found from GridSearchCV and RandomizedSearchCV\nbest_params_grid_search = {'max_depth': 5, 'min_samples_split': 2}\nbest_params_random_search = {'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2}\n\n# List of final models to be evaluated\nfinal_models = {\n    'Decision Tree with GridSearchCV': DecisionTreeClassifier(random_state=42, **best_params_grid_search),\n    'Decision Tree with RandomizedSearchCV': DecisionTreeClassifier(random_state=42, **best_params_random_search),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'Gradient Boosted Trees': GradientBoostingClassifier(random_state=42),\n    'AdaBoost': AdaBoostClassifier(random_state=42),\n    'SVM': SVC(kernel='linear', random_state=42, probability=True)\n}\n\n# Dictionary to store model performance\nmodel_performance = {}\n\n# Evaluate each model\nfor model_name, model in final_models.items():\n    # No PCA in the pipeline for feature importance\n    pipeline = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('classifier', model)\n    ])\n\n    # Performing cross-validation\n    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n    mean_cv_score = np.mean(cv_scores)\n    std_cv_score = np.std(cv_scores)\n\n    # Fitting the pipeline on the full training data\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    # Storing model performance\n    model_performance[model_name] = {\n        'pipeline': pipeline,\n        'cv_score_mean': mean_cv_score,\n        'cv_score_std': std_cv_score,\n        'test_accuracy': accuracy,\n        'confusion_matrix': cm,\n        'classification_report': report\n    }\n\n    print(f\"Model: {model_name}\")\n    print(f\"Cross-Validation Accuracy: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n    print(f\"Test Accuracy: {accuracy}\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\nClassification Report:\\n\", report)\n    print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Finding the best-performing models\nsorted_models = sorted(model_performance.items(), key=lambda x: x[1]['test_accuracy'], reverse=True)\nbest_models = sorted_models[:2]  # Choose the top 2 models\n\n# Plotting feature importance for the best-performing models and learning curves\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n\nfor i, (model_name, performance) in enumerate(best_models):\n    if hasattr(performance['pipeline'].named_steps['classifier'], 'feature_importances_'):\n        feature_importances = performance['pipeline'].named_steps['classifier'].feature_importances_\n\n        # Sorting feature importances in descending order\n        indices = np.argsort(feature_importances)[::-1]\n\n        # Rearranging feature names so they match the sorted feature importances\n        names = [X_train.columns[i] for i in indices]\n\n        ax = axes[i, 0]\n        ax.bar(range(X_train.shape[1]), feature_importances[indices])\n        ax.set_title(f\"Feature Importance for {model_name}\")\n        ax.set_xticks(range(X_train.shape[1]))\n        ax.set_xticklabels(names, rotation=90)\n        ax.set_xlabel('Features')\n        ax.set_ylabel('Importance Score')\n\n        # Displaying the importance scores\n        print(f\"Feature importances for {model_name}:\")\n        for name, importance in zip(names, feature_importances[indices]):\n            print(f\"{name}: {importance:.4f}\")\n\n    # Plotting learning curve for the best-performing models\n    train_sizes, train_scores, test_scores = learning_curve(\n        performance['pipeline'], X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5), scoring='accuracy'\n    )\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax = axes[i, 1]\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    ax.set_title(f\"Learning Curve for {model_name}\")\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")\n    ax.legend(loc=\"best\")\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"756c8df7","outputId":"05db548d-0c42-4b0f-ca75-83be2b3b929d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting learning curves for final models\nplot_learning_curve(final_models['AdaBoost'], \"Learning Curve (AdaBoost)\", X_train, y_train, cv=5, n_jobs=-1)\nplot_learning_curve(final_models['Gradient Boosted Trees'], \"Learning Curve (Gradient Boosted Trees)\", X_train, y_train, cv=5, n_jobs=-1)\n\n#plt.show()","metadata":{"id":"e9572ef5","outputId":"735c6a46-5a50-4f77-f5b3-324a68e6de67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating each model and find the best one based on test accuracy and cross-validation score\nbest_model_name = None\nbest_model = None\nbest_test_accuracy = 0\nbest_cv_score_mean = 0\n\nfor model_name, model in final_models.items():\n    pipeline = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('pca', PCA(n_components=0.95)),  # Retain 95% of the variance\n        ('classifier', model)\n    ])\n\n    # Perform cross-validation\n    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n    mean_cv_score = np.mean(cv_scores)\n    std_cv_score = np.std(cv_scores)\n\n    # Fit the pipeline on the full training data\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    test_accuracy = accuracy_score(y_test, y_pred)\n\n    if test_accuracy > best_test_accuracy and mean_cv_score > best_cv_score_mean:\n        best_test_accuracy = test_accuracy\n        best_cv_score_mean = mean_cv_score\n        best_model_name = model_name\n        best_model = pipeline\n\n    print(f\"Model: {model_name}\")\n    print(f\"Cross-Validation Accuracy: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n    print(f\"Test Accuracy: {test_accuracy}\")\n    print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Printing the best model\nprint(f\"Best Model: {best_model_name}\")\nprint(f\"Best Model Test Accuracy: {best_test_accuracy}\")\nprint(f\"Best Model Cross-Validation Accuracy: {best_cv_score_mean:.4f}\")","metadata":{"id":"21940109","outputId":"a8177ba6-ec20-4e60-f389-3f0d0a0ec3da"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.5. t-SNE for classification visualization","metadata":{"id":"sTLvFX9fcwdO"}},{"cell_type":"code","source":"# t-SNE Visualization for the Best Performing Model\n\n# Assuming best_model is obtained from RandomizedSearchCV\nbest_model = random_search.best_estimator_\n\n# Fitting the best model pipeline to the training data again if needed\nbest_model.fit(X_train, y_train)\n\n# Getting the outputs of the PCA step\npca = best_model.named_steps['pca']\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n# Concatenating the PCA-transformed train and test data\nX_combined_pca = np.vstack((X_train_pca, X_test_pca))\ny_combined = np.concatenate((y_train, y_test))\n\n# Applying t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_combined_pca)\n\n# Creating a DataFrame for the t-SNE result\ntsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])\ntsne_df['Segmentation'] = y_combined\n\n# Plotting the t-SNE result\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x='TSNE1', y='TSNE2', hue='Segmentation', data=tsne_df, palette='viridis', alpha=0.7)\nplt.title('t-SNE Visualization of Segmentation')\nplt.show()","metadata":{"id":"defedd35","outputId":"1e8a8ba7-4b8f-45ce-d12b-e0e0dcb8321b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 2 - Alternative Segmentation \nAnalysis of Train alone, Train Clustering, Test Clustering, Analysis of Alternative Classification","metadata":{"id":"3oWFmapPJIvb"}},{"cell_type":"markdown","source":"# 2.1. Train Dataset Model Evaluation","metadata":{"id":"ziYqW-l6qQu8"}},{"cell_type":"code","source":"# Splitting the dataset\nX = df_train.drop(columns=['Segmentation'])\ny = df_train['Segmentation']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"id":"s2Fa6L_JanWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)","metadata":{"id":"jcjTwNumb0jx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of classification models to evaluate\nmodels = {\n    'RandomForest': RandomForestClassifier(random_state=42),\n    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n    'AdaBoost': AdaBoostClassifier(random_state=42),\n    'ExtraTrees': ExtraTreesClassifier(random_state=42),\n    'Bagging': BaggingClassifier(random_state=42),\n    'DecisionTree': DecisionTreeClassifier(random_state=42),\n    'SVM': SVC(kernel='linear', random_state=42),\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'KNN': KNeighborsClassifier(),\n    'XGBoost': XGBClassifier(random_state=42)\n}","metadata":{"id":"6eQEinXdannm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Evaluation\ndef evaluate_model(model, X_train, y_train, X_val, y_val):\n    try:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        accuracy = accuracy_score(y_val, y_pred)\n        cm = confusion_matrix(y_val, y_pred)\n        cr = classification_report(y_val, y_pred)\n        return accuracy, cm, cr\n    except Exception as e:\n        print(f\"Error evaluating model {model}: {e}\")\n        return None, None, None\n\n# Evaluate each model and print results\nresults = {}\nfor name, model in models.items():\n    accuracy, cm, cr = evaluate_model(model, X_train_scaled, y_train, X_val_scaled, y_val)\n    if accuracy is not None:\n        results[name] = {\n            'accuracy': accuracy,\n            'confusion_matrix': cm,\n            'classification_report': cr\n        }","metadata":{"id":"LSR1p7D_CwFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the best model\nbest_model_name = max(results, key=lambda x: results[x]['accuracy'])\nbest_model_accuracy = results[best_model_name]['accuracy']\nbest_model_cm = results[best_model_name]['confusion_matrix']\n\nprint(f'Best Model: {best_model_name}')\nprint(f'Accuracy: {best_model_accuracy}')\nprint(f'Confusion Matrix:\\n{best_model_cm}')","metadata":{"id":"hszhYoHOf_2e","outputId":"a80db0c9-5a7e-4e07-e6d6-13e76845fe23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter tuning for improving prediction model performance\n# Defining a parameter grid for GridSearchCV\nparam_grid = {\n    'n_estimators': [100, 150],\n    'learning_rate': [0.1, 0.2],\n    'max_depth': [3, 4],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\n# Initializing the Gradient Boosting model\ngb_model = GradientBoostingClassifier(random_state=42)\n\n# Initializing GridSearchCV\ngrid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid,\n                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n\n# Fitting GridSearchCV\ngrid_search.fit(X_train_scaled, y_train)\n\n# Getting the best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\n# Evaluating the best model on the validation set\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_val_scaled)\naccuracy = accuracy_score(y_val, y_pred)\ncm = confusion_matrix(y_val, y_pred)\ncr = classification_report(y_val, y_pred)\n\nprint(f'Best Parameters: {best_params}')\nprint(f'Best Score: {best_score}')\nprint(f'Accuracy: {accuracy}')\nprint(f'Confusion Matrix:\\n{cm}')\nprint(f'Classification Report:\\n{cr}')","metadata":{"id":"qVRWkvfsF7GD","outputId":"65e67e48-8e89-476b-a227-7144eb1fbdeb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2. t-SNE Visualization of the Train Data","metadata":{"id":"V0_o32h6qWGK"}},{"cell_type":"code","source":"# t-SNE for Data Visualization\n# Converting scaled data back to DataFrame for easier handling\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_train_scaled_df['Segmentation'] = y_train.values\n\n# Applying t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X_train_scaled)\n\n# Plotting the t-SNE output\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_train, cmap='viridis', alpha=0.6)\nplt.title('t-SNE visualization of the Data')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.colorbar(scatter)\nplt.show()\n\nplt.figure(figsize=(10,8))\nsns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue='Segmentation',\n               palette=sns.color_palette('hls', len(y_train.unique())),\n               data=X_train_scaled_df,\n               alpha=0.6)\nplt.title('t-SNE visualization with Hue by Original Segmentation')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()","metadata":{"id":"pnxo5W0YF7Wc","outputId":"ab2600ff-cf9a-4fa7-ee98-ad79cdd664f6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3. Train Dataset Clustering","metadata":{"id":"7q3Yly3TziBt"}},{"cell_type":"code","source":"# Dropping the target variable 'Segmentation' and create a new DataFrame for clustering\ndf_clustering = df_train.drop(columns=['Segmentation'])","metadata":{"id":"FmrsNmq9-1QY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing the data\nscaler = StandardScaler()\nX_clustering_scaled = scaler.fit_transform(df_clustering)","metadata":{"id":"XJzYGOYlF77V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clustering using K-means and Agglomerative Clustering\n# Applying K-Means Clustering\nkmeans = KMeans(n_clusters=4, random_state=42)\nkmeans_labels = kmeans.fit_predict(X_clustering_scaled)\n\n# Applying Agglomerative Clustering\nagg_clustering = AgglomerativeClustering(n_clusters=4)\nagg_labels = agg_clustering.fit_predict(X_clustering_scaled)","metadata":{"id":"orBLgL0whR86","outputId":"8ad74ca8-7a61-4ab3-c909-ca7292ad9613"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t-SNE visualization of K-Means clusters\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_clustering_scaled)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)\nplt.title('t-SNE visualization of K-Means Clusters')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.colorbar(scatter)\nplt.show()\n\n# t-SNE visualization of Agglomerative Clusters\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=agg_labels, cmap='viridis', alpha=0.6)\nplt.title('t-SNE visualization of Agglomerative Clusters')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.colorbar(scatter)\nplt.show()","metadata":{"id":"tWOjwgaZF8LO","outputId":"347fb788-2f3f-47e7-e724-221879cd9851"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dendrogram for Hierarchical Clustering\nlinked = linkage(X_clustering_scaled, 'ward')\n\nplt.figure(figsize=(10, 7))\ndendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\nplt.title('Dendrogram for Hierarchical Clustering')\nplt.xlabel('Sample index')\nplt.ylabel('Distance')\nplt.show()","metadata":{"id":"jrmvV_AG4pyk","outputId":"3aa77a5e-eaf7-43f6-f027-ba85e396bedb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Silhouette Score for K-Means Clustering\nkmeans_labels = kmeans.fit_predict(X_clustering_scaled)\nkmeans_silhouette = silhouette_score(X_clustering_scaled, kmeans_labels)\nprint(f'Silhouette Score for K-Means: {kmeans_silhouette:.4f}')","metadata":{"id":"7rUO5j8MCehZ","outputId":"c928d041-8534-4c29-fbfc-f98b15948fb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Silhouette Score for Agglomerative Clustering\nagg_labels = agg_clustering.fit_predict(X_clustering_scaled)\nagg_silhouette = silhouette_score(X_clustering_scaled, agg_labels)\nprint(f'Silhouette Score for Agglomerative Clustering: {agg_silhouette:.4f}')","metadata":{"id":"oFHCSx8sCehZ","outputId":"b66f3bda-e635-4885-d801-20300e2fbf21"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.4. Train Dataset Model Evaluation with New Segmentation","metadata":{"id":"I1Tcptd_qigx"}},{"cell_type":"code","source":"# Appending the cluster labels to the train dataframe\ndf_train['KMeans_Cluster'] = kmeans_labels\ndf_train['Agglomerative_Cluster'] = agg_labels\n\n# Creating the 'Alt_Seg' column based on the clustering results using Agglomerative Clustering\ndf_train['Alt_Seg'] = agg_labels\n\nprint(df_train.head())","metadata":{"id":"jTm7xVyO4qB2","outputId":"fd0cf9b0-bac4-42fd-df1c-185afc3c80ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing 'Segmentation' and Clustering Columns\ndf_train = df_train.drop(columns=['KMeans_Cluster', 'Agglomerative_Cluster', 'Segmentation'])\n\n# Defining the new features and target variable\nX_alt = df_train.drop(columns=['Alt_Seg'])\ny_alt = df_train['Alt_Seg']\n\n# Splitting the data into training and validation sets\nX_alt_train, X_alt_val, y_alt_train, y_alt_val = train_test_split(X_alt, y_alt, test_size=0.2, random_state=42)\n\n# Standardizing the data\nscaler = StandardScaler()\nX_alt_train_scaled = scaler.fit_transform(X_alt_train)\nX_alt_val_scaled = scaler.transform(X_alt_val)\n\n# Training a model using RandomForestClassifier\nalt_model = RandomForestClassifier(random_state=42)\nalt_model.fit(X_alt_train_scaled, y_alt_train)\n\n# Predicting the new segmentation on the validation set\ny_alt_pred = alt_model.predict(X_alt_val_scaled)\n\n# Evaluating the model\nalt_accuracy = accuracy_score(y_alt_val, y_alt_pred)\nalt_cm = confusion_matrix(y_alt_val, y_alt_pred)\nalt_cr = classification_report(y_alt_val, y_alt_pred)\n\nprint(f'Accuracy of the model with Alt_Seg: {alt_accuracy}')\nprint(f'Confusion Matrix:\\n{alt_cm}')\nprint(f'Classification Report:\\n{alt_cr}')","metadata":{"id":"OfpDpIp34qSZ","outputId":"80e8e39b-c40f-4148-bfe4-d623898eb5dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing new classification\n# Counting the number of unique classification values\nnum_classes = df_train['Alt_Seg'].nunique()\nprint(f'Number of unique classification values: {num_classes}')\n\n# Counting the number of customers in each 'Alt_Seg' type\nclass_counts = df_train['Alt_Seg'].value_counts()\nprint(f'Number of customers in each Alt_Seg type:\\n{class_counts}')\n\n# Summary statistics for each 'Alt_Seg' type\nclass_summary = df_train.groupby('Alt_Seg').agg({\n    'Work_Experience': ['mean', 'median', 'std'],\n    'Family_Size': ['mean', 'median', 'std'],\n    'Gender': lambda x: x.value_counts().idxmax(),\n    'Ever_Married': lambda x: x.value_counts().idxmax(),\n    'Graduated': lambda x: x.value_counts().idxmax(),\n    'Spending_Score': lambda x: x.value_counts().idxmax(),\n    'Var_1': lambda x: x.value_counts().idxmax(),\n})\n\nprint(f'Summary statistics for each Alt_Seg type:\\n{class_summary}')","metadata":{"id":"HlJ3OmlV4qn6","outputId":"4a8d99dd-d0f5-4c40-b261-ad1294b364ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar plot of customer distribution across segments\nplt.figure(figsize=(10, 6))\ndf_train['Alt_Seg'].value_counts().plot(kind='bar', color='skyblue')\nplt.title('Customer Distribution Across Segments')\nplt.xlabel('Segments')\nplt.ylabel('Number of Customers')\nplt.show()","metadata":{"id":"XLZC80Y07fza","outputId":"de20b054-4a9a-40f0-e803-b44e37859199"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pie chart of customer distribution\nplt.figure(figsize=(8, 8))\ndf_train['Alt_Seg'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=sns.color_palette('pastel'))\nplt.title('Customer Distribution Across Segments')\nplt.ylabel('')\nplt.show()","metadata":{"id":"Z4CgN2uq7gTE","outputId":"8203f224-82d7-4989-ba13-08947b586d93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box plots of numerical features across segments\nnumerical_features = ['Work_Experience', 'Family_Size']\nfor feature in numerical_features:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Alt_Seg', y=feature, data=df_train)\n    plt.title(f'{feature} Distribution Across Segments')\n    plt.xlabel('Segments')\n    plt.ylabel(feature)\n    plt.show()","metadata":{"id":"oUVbCwEz7glx","outputId":"2b1185f7-192f-421a-8e0c-0ac4ab67c4c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detailed profiles for each segment\nfor seg in df_train['Alt_Seg'].unique():\n    segment_data = df_train[df_train['Alt_Seg'] == seg]\n    print(f'\\nProfile for Segment {seg}:')\n    print(segment_data.describe(include='all'))","metadata":{"id":"eb19XOjD7hHZ","outputId":"c11e7eaf-5dca-4c7e-f62f-e87c4733697b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importance\nfeature_importances = alt_model.feature_importances_\n\n# Getting feature names\nfeature_names = X_alt.columns\n\n# Creating a DataFrame for visualization\nfeature_importances_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n}).sort_values(by='Importance', ascending=False)\n\n# Plotting feature importance\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importances_df, palette='viridis')\nplt.title('Feature Importance from RandomForestClassifier', fontsize=20, fontweight='bold')\nplt.xlabel('Importance', fontsize=14)\nplt.ylabel('Features', fontsize=14)\nplt.tight_layout()\nplt.show()","metadata":{"id":"2AEbNg2Z7hYz","outputId":"b417be82-72f5-481a-ad18-213b1800d341"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating prediction models with new segmentation\n# Splitting the dataset\nX = df_train.drop(columns=['Alt_Seg'])\ny = df_train['Alt_Seg']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"id":"p-WmeHE0jota"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)","metadata":{"id":"mV6WCy-rjuwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of models to evaluate\nmodels = {\n    'RandomForest': RandomForestClassifier(random_state=42),\n    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n    'AdaBoost': AdaBoostClassifier(random_state=42),\n    'ExtraTrees': ExtraTreesClassifier(random_state=42),\n    'Bagging': BaggingClassifier(random_state=42),\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'KNN': KNeighborsClassifier(),\n    'XGBoost': XGBClassifier(random_state=42)\n}\n\ndef evaluate_model(model, X_train, y_train, X_val, y_val):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    accuracy = accuracy_score(y_val, y_pred)\n    cm = confusion_matrix(y_val, y_pred)\n    cr = classification_report(y_val, y_pred)\n    return accuracy, cm, cr\n\n# Evaluating each model\nresults = {}\nfor name, model in models.items():\n    accuracy, cm, cr = evaluate_model(model, X_train_scaled, y_train, X_val_scaled, y_val)\n    results[name] = {\n        'accuracy': accuracy,\n        'confusion_matrix': cm,\n        'classification_report': cr\n    }\n\n# Finding the best model\nbest_model_name = max(results, key=lambda x: results[x]['accuracy'])\nbest_model_accuracy = results[best_model_name]['accuracy']\nbest_model_cm = results[best_model_name]['confusion_matrix']\n\nprint(f'Best Model: {best_model_name}')\nprint(f'Accuracy: {best_model_accuracy}')\nprint(f'Confusion Matrix:\\n{best_model_cm}')","metadata":{"id":"Vc2oa3Y8FPej","outputId":"4f23f3d6-0395-40bd-91c9-e9e383f3f33e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.5. Applying Clustering to the Test Dataset","metadata":{"id":"G584xzOOj7Wq"}},{"cell_type":"code","source":"# Removing 'Segmentation' to apply new clustering on test_df\ndf_test = df_test.drop(columns=['Segmentation'])","metadata":{"id":"tJ79Pf4bkZRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensuring 'Test' dataset has the same columns as 'Train' dataset\nmissing_cols = set(X_train.columns) - set(df_test.columns)\nfor col in missing_cols:\n    df_test[col] = 0  # Adding missing columns with default value 0\n\n# Ensuring the same order of columns\ndf_test = df_test[X_train.columns]\n\nprint(df_test.head())","metadata":{"id":"L6x-sumnkZGU","outputId":"b4a3d99e-2de9-4792-b8fe-6bdfd3828fce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing the 'Test' data\nX_test_scaled = scaler.transform(df_test)\n\n# Predicting the 'Alt_Seg' on the 'Test' dataset using the best model\ny_test_pred = best_model.predict(X_test_scaled)\n\n# Creating a new dataframe with the predictions\ndf_test['Alt_Seg'] = y_test_pred\n\n# Saving the predictions to a CSV file\ndf_test.to_csv('Test_with_Alt_Seg.csv', index=False)\n\nprint(df_test.head())","metadata":{"id":"obl1VqrrnUiH","outputId":"d8596c2a-d22f-481f-a322-1d2360fa223e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of each predicted segment\nalt_seg_counts = df_test['Alt_Seg'].value_counts()\nprint(\"Count of each predicted Alt_Seg:\")\nprint(alt_seg_counts)\n\n# Convert counts to string for title\ncounts_str = ', '.join([f'Alt_Seg {seg}: {count}' for seg, count in alt_seg_counts.items()])\n\n# Visualizing the distribution of the predicted labels\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Alt_Seg', data=df_test, palette='viridis', hue='Alt_Seg', dodge=False, legend=False)\nplt.title(f'Distribution of Predicted Alt_Seg in Test Dataset')\nplt.xlabel('Alt_Seg')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"id":"99GYBwRPnULP","outputId":"12361abd-fec8-4d44-f7c5-cca041f2efb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pie chart of customer distribution across segments in test data\nplt.figure(figsize=(8, 8))\ndf_test['Alt_Seg'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=sns.color_palette('pastel'))\nplt.title('Customer Distribution Across Segments in Test Data')\nplt.ylabel('')\nplt.show()","metadata":{"id":"WiVE492JnxD_","outputId":"0f5ad8e8-765f-4df6-ff25-34268566bdff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating silhouette score\nsilhouette_avg = silhouette_score(X_test_scaled, y_test_pred)\nprint(f'Silhouette Score for the clustering on test data: {silhouette_avg}')","metadata":{"id":"KXAleQ8PnUCj","outputId":"dfb58547-47fa-47e3-a6fa-9b2023326f90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting summary statistics\ntest_class_summary = df_test.groupby('Alt_Seg').agg({\n    'Work_Experience': ['mean', 'median', 'std'],\n    'Family_Size': ['mean', 'median', 'std'],\n    'Gender': lambda x: x.value_counts().idxmax(),\n    'Ever_Married': lambda x: x.value_counts().idxmax(),\n    'Graduated': lambda x: x.value_counts().idxmax(),\n    'Spending_Score': lambda x: x.value_counts().idxmax(),\n    'Var_1': lambda x: x.value_counts().idxmax(),\n})\n\nprint(f'Summary statistics for each Alt_Seg type in test data:\\n{test_class_summary}')","metadata":{"id":"B2DQ1PDqnT7O","outputId":"b263c231-470b-489b-8670-343a73d012f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box plots of numerical features in test data, by segment\nnumerical_features = ['Work_Experience', 'Family_Size']\nfor feature in numerical_features:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Alt_Seg', y=feature, data=df_test)\n    plt.title(f'{feature} Distribution Across Segments in Test Data')\n    plt.xlabel('Segments')\n    plt.ylabel(feature)\n    plt.show()\n","metadata":{"id":"6X9FDuEYnTyp","outputId":"15cd4b2a-de56-4bf0-89fe-d9c6678120a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t-SNE to the test data\ntsne_test = TSNE(n_components=2, random_state=42)\nX_tsne_test = tsne_test.fit_transform(X_test_scaled)\n\n# t-SNE visualization of agglomerative clusters\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne_test[:, 0], X_tsne_test[:, 1], c=y_test_pred, cmap='viridis', alpha=0.6)\nplt.title('t-SNE visualization of Test Data Clusters')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.colorbar(scatter)\nplt.show()","metadata":{"id":"ZodnAENfnyX_","outputId":"148f2f41-b2c6-4be7-8cc0-c7a4a5c3886e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.6. Evaluating Model on the Test Dataset with the New Segmentation","metadata":{"id":"VH7NRUNvoeLq"}},{"cell_type":"code","source":"# Defining df as train and test_df as test to apply models and see fit of the new classification on test\nX_train_1 = df_train.drop(columns=['Alt_Seg'])\ny_train_1 = df_train['Alt_Seg']\n\nX_test_1 = df_test.drop(columns=['Alt_Seg'])\ny_test_1 = df_test['Alt_Seg']","metadata":{"id":"COl8Ptz2nyRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing the data\nscaler_1 = StandardScaler()\nX_train_scaled_1 = scaler_1.fit_transform(X_train_1)\nX_test_scaled_1 = scaler_1.transform(X_test_1)","metadata":{"id":"QiN1Kvz3n7UW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nmodel_1 = GradientBoostingClassifier(random_state=42)\nmodel_1.fit(X_train_scaled_1, y_train_1)\n\n# Predicting on the test set\ny_test_pred_1 = model_1.predict(X_test_scaled_1)\n\n# Evaluating the model\naccuracy_1 = accuracy_score(y_test_1, y_test_pred_1)\ncm_1 = confusion_matrix(y_test_1, y_test_pred_1)\ncr_1 = classification_report(y_test_1, y_test_pred_1)\n\nprint(f'Accuracy of GradientBoostingClassifier on test data: {accuracy_1}')\nprint(f'Confusion Matrix:\\n{cm_1}')\nprint(f'Classification Report:\\n{cr_1}')","metadata":{"id":"hoVA97zJn7J7","outputId":"5b7f0088-3a1f-4fa3-919d-6fafe3e138c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary statistics\ntest_class_summary_1 = df_test.groupby('Alt_Seg').agg({\n    'Work_Experience': ['mean', 'median', 'std'],\n    'Family_Size': ['mean', 'median', 'std'],\n    'Gender': lambda x: x.value_counts().idxmax(),\n    'Ever_Married': lambda x: x.value_counts().idxmax(),\n    'Graduated': lambda x: x.value_counts().idxmax(),\n    'Spending_Score': lambda x: x.value_counts().idxmax(),\n    'Var_1': lambda x: x.value_counts().idxmax(),\n})\nprint(f'Summary statistics for each Alt_Seg type in test data:\\n{test_class_summary_1}')","metadata":{"id":"qBWpVGMhn6-m","outputId":"a44f34cd-a9f8-464c-c98d-59b2665c9e3f"},"execution_count":null,"outputs":[]}]}